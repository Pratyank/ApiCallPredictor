# OpenSesame Predictor - Assumptions and Design Decisions

## Core System Assumptions

### 1. Performance Requirements
- **Response Time Target**: < 800ms for end-to-end prediction pipeline
- **LLM Latency**: < 500ms for AI-based predictions (Anthropic Claude)
- **ML Scoring**: < 100ms for LightGBM ranking with k+buffer strategy
- **Cache Hit Rate**: > 80% for common usage patterns
- **Concurrent Users**: Support for 20+ simultaneous users
- **Memory Usage**: < 1GB per service instance

### 2. User Interaction Patterns
- **English Language Primary**: System optimized for English prompts and API documentation
- **Conversational Context**: Users provide 1-10 previous API calls as context
- **Session Length**: Typical user sessions last 5-30 minutes with 3-15 API predictions
- **Prompt Complexity**: User prompts range from 10-500 words, averaging 50-100 words
- **Prediction Usage**: Users typically use top 1-3 predictions, with 85% success rate

### 3. API Ecosystem Assumptions
- **REST API Focus**: Primary focus on RESTful API patterns (GET, POST, PUT, DELETE)
- **OpenAPI Specification**: APIs follow OpenAPI 3.x standards where available
- **Standard HTTP Methods**: APIs use conventional HTTP methods and response codes
- **JSON Data Format**: API requests/responses primarily use JSON format
- **Resource-Based Design**: APIs follow resource-based URL patterns (/api/resource/{id})

### 4. Training Data Assumptions
- **Synthetic Training Data**: Initial ML training uses 10,000 generated SaaS workflow sequences
- **Markov Chain Patterns**: Workflow patterns follow realistic SaaS application usage
- **Positive/Negative Examples**: 70% positive examples (actual next API call) vs 30% negative examples
- **Feature Relevance**: 11 engineered features provide sufficient signal for ranking
- **Pattern Stability**: API usage patterns remain relatively stable over time

### 5. AI Provider Assumptions
- **Anthropic Claude API Stability**: Primary AI provider with 99.9% uptime assumption
- **API Rate Limits**: Anthropic API supports required throughput (10+ RPS)
- **Response Quality**: Claude 3 Haiku provides consistent, high-quality predictions
- **Cost Efficiency**: Anthropic pricing remains cost-effective for production use
- **Fallback Reliability**: OpenAI GPT-3.5 serves as reliable secondary provider

## New Assumptions for Cost-Aware Model Router

### 6. Anthropic Model Pricing Stability
- **Claude 3 Haiku Pricing**: $0.25 per 1M input tokens, $1.25 per 1M output tokens
- **Price Stability Period**: Pricing expected to remain stable for 6-12 months
- **Cost-Performance Trade-off**: Haiku provides optimal balance of speed, accuracy, and cost
- **Volume Discounts**: Enterprise pricing may provide 10-20% cost reduction at scale
- **Regional Pricing**: US pricing used as baseline, with Â±10% variation for other regions

### 7. Cost-Aware Routing Logic
- **Cost Threshold**: Switch to cheaper models when cost exceeds $0.01 per prediction
- **Quality Threshold**: Maintain minimum 80% prediction accuracy regardless of cost
- **Model Selection Criteria**: Balance cost, latency, and accuracy in model selection
- **Fallback Strategy**: Degrade gracefully to cheaper models under budget constraints
- **Cost Monitoring**: Track and alert on cost anomalies and budget threshold breaches

### 8. Model Performance Assumptions
- **Haiku Response Time**: Consistent 200-500ms response times for prediction requests
- **Sonnet Fallback**: Claude 3 Sonnet available for complex queries requiring higher accuracy
- **Token Usage Patterns**: Typical predictions use 100-300 input tokens, 50-150 output tokens
- **Request Batching**: Single API calls sufficient, no need for batch processing
- **Error Rate**: < 1% API error rate under normal operating conditions

### 9. Budget and Financial Assumptions
- **Monthly Budget**: $100-1000 monthly budget for AI API costs depending on usage
- **Cost per Prediction**: Target cost of $0.001-0.005 per prediction including all AI costs
- **Budget Alerting**: 75% budget threshold triggers cost optimization measures
- **Cost Attribution**: Costs tracked per user/session for accurate billing and optimization
- **Financial Monitoring**: Daily cost tracking with weekly trend analysis

### 10. Integration and Deployment Assumptions
- **Environment Variables**: API keys and configuration managed via environment variables
- **Configuration Changes**: Cost thresholds and model selection configurable without code changes
- **Monitoring Integration**: Cost and performance metrics integrated into existing monitoring
- **Backward Compatibility**: CostAwareRouter maintains compatibility with existing AiLayer interface
- **Gradual Rollout**: New routing logic deployed gradually with feature flags

## Technical Architecture Assumptions

### 11. Database and Storage
- **SQLite Performance**: SQLite3 sufficient for single-instance deployment with < 10,000 endpoints
- **Data Persistence**: Cache data survives 1-hour TTL with automated cleanup
- **Concurrent Access**: SQLite handles concurrent read/write operations for < 50 RPS
- **Backup Strategy**: Daily database backups sufficient for disaster recovery
- **Schema Evolution**: Database schema changes managed via migration scripts

### 12. Security and Compliance
- **Input Validation**: All user inputs validated and sanitized before processing
- **Rate Limiting**: 60 requests per minute per user prevents abuse
- **PII Protection**: Personal information automatically detected and filtered
- **API Key Security**: AI provider API keys stored securely and rotated regularly
- **Audit Logging**: All prediction requests logged for security analysis

### 13. Scalability and Performance
- **Horizontal Scaling**: Load balancing across multiple instances for > 100 RPS
- **Cache Distribution**: Redis cluster for distributed caching in scaled deployments
- **Database Scaling**: PostgreSQL migration path for > 100,000 endpoints
- **Auto-scaling**: Container orchestration handles traffic spikes automatically
- **Performance Monitoring**: Real-time performance metrics guide scaling decisions

### 14. Maintenance and Operations
- **Model Retraining**: ML models retrained weekly with new interaction data
- **Endpoint Updates**: OpenAPI specifications refreshed daily with 1-hour TTL
- **Health Monitoring**: Comprehensive health checks ensure 99.9% uptime
- **Error Recovery**: Automatic retry logic and graceful degradation for component failures
- **Version Management**: Rolling deployments with automated rollback capabilities

## Risk Mitigation Assumptions

### 15. AI Provider Dependencies
- **Provider Diversification**: Multiple AI providers reduce single point of failure
- **Cost Escalation**: Budget monitoring and automatic cost controls prevent overspend
- **API Changes**: Provider API changes handled via adapter pattern with version management
- **Regional Availability**: Multi-region deployment reduces geographic dependencies
- **Contract Negotiations**: Enterprise agreements provide cost predictability and SLA guarantees

### 16. Data Quality and Accuracy
- **Synthetic Data Quality**: Generated training data represents real-world usage patterns
- **Model Drift**: Regular performance monitoring detects and corrects model degradation
- **Feedback Loops**: User interactions provide continuous training signal
- **Ground Truth**: Manual validation of predictions maintains quality baselines
- **A/B Testing**: Controlled experiments validate model improvements

### 17. Security and Privacy
- **Data Anonymization**: User data anonymized before storage or analysis
- **Encryption Standards**: All data encrypted in transit and at rest
- **Access Controls**: Role-based access controls protect sensitive functionality
- **Compliance**: GDPR, SOC2, and industry-specific compliance requirements met
- **Incident Response**: Security incident response procedures documented and tested

## Future Evolution Assumptions

### 18. Technology Roadmap
- **AI Model Evolution**: New AI models integrated via plugin architecture
- **Cost Optimization**: Advanced cost optimization techniques (request batching, caching)
- **Performance Improvements**: Sub-500ms total response time achievable with optimization
- **Feature Expansion**: Additional ML features improve prediction accuracy over time
- **Integration Ecosystem**: Third-party integrations expand platform capabilities

### 19. Business Requirements
- **User Growth**: System scales to support 10,000+ daily active users
- **Enterprise Features**: Advanced analytics, custom models, and white-label options
- **Revenue Model**: Usage-based pricing with tiered service levels
- **Global Expansion**: Multi-language support and regional deployment
- **Partner Ecosystem**: API marketplace and developer platform integration

These assumptions guide system design decisions and provide a framework for evaluating trade-offs between performance, cost, accuracy, and scalability. Regular review and validation of these assumptions ensures the system continues to meet evolving requirements.